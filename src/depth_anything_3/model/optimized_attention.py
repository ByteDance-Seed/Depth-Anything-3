#!/usr/bin/env python3
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: Apache-2.0

"""
Optimized attention implementation with automatic backend selection.

This module provides platform-optimized attention mechanisms:
- CUDA: Uses PyTorch's fused scaled_dot_product_attention
- MPS (Apple Silicon): Uses manual implementation (2-3x faster than PyTorch's)
- CPU: Uses PyTorch's implementation

The manual implementation is significantly faster on MPS because PyTorch's
scaled_dot_product_attention has poor MPS backend optimization.
"""

import torch
import torch.nn.functional as F
from torch import Tensor
import logging

logger = logging.getLogger(__name__)


def should_use_manual_attention(device: torch.device) -> bool:
    """
    Determine if manual attention implementation should be used.

    Args:
        device: The device where computation will happen

    Returns:
        True if manual implementation should be used (MPS), False otherwise
    """
    # Use manual implementation on MPS for 2-3x speedup
    # PyTorch's scaled_dot_product_attention is poorly optimized on MPS
    return device.type == "mps"


def scaled_dot_product_attention_optimized(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    attn_mask: Tensor | None = None,
    dropout_p: float = 0.0,
    scale: float | None = None,
    training: bool = False,
) -> Tensor:
    """
    Optimized scaled dot-product attention with automatic backend selection.

    Automatically chooses between:
    - PyTorch's F.scaled_dot_product_attention (CUDA, CPU)
    - Manual implementation (MPS - 2-3x faster)

    Args:
        q: Query tensor (B, H, N, D)
        k: Key tensor (B, H, N, D)
        v: Value tensor (B, H, N, D)
        attn_mask: Optional attention mask
        dropout_p: Dropout probability
        scale: Optional scale factor (defaults to 1/sqrt(head_dim))
        training: Whether in training mode

    Returns:
        Attention output tensor (B, H, N, D)
    """
    device = q.device

    # Determine which implementation to use
    use_manual = should_use_manual_attention(device)

    if use_manual:
        # Manual implementation (optimized for MPS)
        return _manual_scaled_dot_product_attention(
            q, k, v, attn_mask, dropout_p, scale, training
        )
    else:
        # Use PyTorch's optimized implementation
        return F.scaled_dot_product_attention(
            q, k, v,
            attn_mask=attn_mask,
            dropout_p=dropout_p if training else 0.0,
            scale=scale,
        )


def _manual_scaled_dot_product_attention(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    attn_mask: Tensor | None = None,
    dropout_p: float = 0.0,
    scale: float | None = None,
    training: bool = False,
) -> Tensor:
    """
    Manual implementation of scaled dot-product attention.

    This implementation is significantly faster on MPS (Apple Silicon) than
    PyTorch's native scaled_dot_product_attention.

    Computes: attention(Q,K,V) = softmax(QK^T/âˆšd)V

    Args:
        q: Query tensor (B, H, N_q, D)
        k: Key tensor (B, H, N_k, D)
        v: Value tensor (B, H, N_k, D)
        attn_mask: Optional attention mask
        dropout_p: Dropout probability
        scale: Optional scale factor (defaults to 1/sqrt(head_dim))
        training: Whether in training mode

    Returns:
        Attention output tensor (B, H, N_q, D)
    """
    # Calculate scale if not provided
    if scale is None:
        head_dim = q.size(-1)
        scale = head_dim ** -0.5

    # Compute attention scores: QK^T
    attn = torch.matmul(q, k.transpose(-2, -1))

    # Scale
    attn = attn * scale

    # Apply attention mask if provided
    if attn_mask is not None:
        attn = attn + attn_mask

    # Softmax
    attn = F.softmax(attn, dim=-1)

    # Apply dropout if training
    if training and dropout_p > 0.0:
        attn = F.dropout(attn, p=dropout_p, training=training)

    # Apply attention to values
    output = torch.matmul(attn, v)

    return output


# Log which backend will be used at module import time
_device_type = "cuda" if torch.cuda.is_available() else (
    "mps" if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else "cpu"
)
_will_use_manual = should_use_manual_attention(torch.device(_device_type))

if _will_use_manual:
    logger.info(
        f"Optimized attention: using manual implementation for {_device_type.upper()} "
        "(2-3x faster than PyTorch's scaled_dot_product_attention)"
    )
else:
    logger.info(
        f"Optimized attention: using PyTorch's scaled_dot_product_attention for {_device_type.upper()}"
    )
